{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import itertools as it\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "#Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# make prettier plots\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "#Seaborn\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "#Sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.utils import resample\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             f1_score,\n",
    "                             roc_auc_score,\n",
    "                             roc_curve,\n",
    "                             confusion_matrix)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import (RandomForestClassifier,\n",
    "                              RandomForestClassifier,\n",
    "                              RandomForestRegressor,\n",
    "                              GradientBoostingClassifier, \n",
    "                              ExtraTreesClassifier, #For each feature split rule is random, not optimal\n",
    "                              VotingClassifier, \n",
    "                              AdaBoostClassifier, \n",
    "                              BaggingRegressor)\n",
    "\n",
    "from sklearn.model_selection import (cross_val_score,\n",
    "                                     GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     learning_curve,\n",
    "                                     validation_curve,\n",
    "                                     train_test_split,\n",
    "                                     cross_validate)\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Corpus of News Article Text\n",
    "# with open('./data/news_df.pickle', 'rb') as file:\n",
    "# with open('./data/news_data_frame_reduced_preprocessed.pickle', 'rb') as file:\n",
    "with open('./data/news_fake_real_df_reduced_token.pickle', 'rb') as file:\n",
    "    news_data_frame = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     print(news_data_frame.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare text grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text and response to array \n",
    "x_text = news_data_frame.text.values\n",
    "y_response = news_data_frame.Not_Real_or_Real.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to recombine nested list of tokens into full articles \n",
    "def lemma_combine(lis):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    for i in range(len(lis)):\n",
    "        concat_art = ' '.join(lis[i])\n",
    "        parsed_articles.append(concat_art)\n",
    "    \n",
    "    return parsed_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to match \"cleaned\" text back up with response variable\n",
    "def zip_response(observations, response):\n",
    "    response = response.tolist()\n",
    "    \n",
    "    return list(zip(observations, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = news_data_frame['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-24 14:08:40,936 : INFO : collecting all words and their counts\n",
      "2019-05-24 14:08:40,938 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-05-24 14:08:44,825 : INFO : collected 1021646 word types from a corpus of 1797618 words (unigram + bigrams) and 4426 sentences\n",
      "2019-05-24 14:08:44,825 : INFO : using 1021646 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-05-24 14:08:44,827 : INFO : source_vocab length 1021646\n",
      "2019-05-24 14:08:57,928 : INFO : Phraser built with 16848 16848 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "# Create bi-grams for our text \n",
    "phrases = Phrases(lem)\n",
    "bigram = Phraser(phrases)\n",
    "bigram_lem = list(bigram[lem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-24 14:09:03,463 : INFO : collecting all words and their counts\n",
      "2019-05-24 14:09:03,465 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-05-24 14:09:06,635 : INFO : collected 1067409 word types from a corpus of 1589332 words (unigram + bigrams) and 4426 sentences\n",
      "2019-05-24 14:09:06,636 : INFO : using 1067409 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-05-24 14:09:06,636 : INFO : source_vocab length 1067409\n",
      "2019-05-24 14:09:21,594 : INFO : Phraser built with 22256 22256 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "# Create tri-grams for our text\n",
    "phrases2 = Phrases(bigram_lem)\n",
    "trigram = Phraser(phrases2)\n",
    "trigram_lem = list(trigram[bigram_lem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recombine full article text for unigrams, bigrams, and trigrams\n",
    "uni_lem_comb = lemma_combine(lem)\n",
    "bi_lem_comb = lemma_combine(bigram_lem)\n",
    "tri_lem_comb = lemma_combine(trigram_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_uni = \"./data/grams/token_unigram_text\"\n",
    "fileObject = open(token_uni,'wb') \n",
    "pickle.dump(lem,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_trigrams = \"./data/grams/token_trigram_text\"\n",
    "fileObject = open(token_trigrams,'wb') \n",
    "pickle.dump(trigram_lem,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed text so it can be loaded later \n",
    "unigrams = \"./data/grams/unigram_text\"\n",
    "fileObject = open(unigrams,'wb') \n",
    "pickle.dump(uni_lem_comb,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = \"./data/grams/bigram_text\"\n",
    "fileObject = open(bigrams,'wb') \n",
    "pickle.dump(bi_lem_comb,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = \"./data/grams/trigram_text\"\n",
    "# open the file for writing\n",
    "fileObject = open(trigrams,'wb')\n",
    "pickle.dump(tri_lem_comb,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
