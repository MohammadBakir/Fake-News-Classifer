{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "# Setup nltk corpora path and Google Word2Vec location\n",
    "google_vec_file = r\"C:\\Users\\moham\\Metis Bootcamp\\GoogleNewsVectors\\GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)\n",
    "\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Corpus of News Article Text\n",
    "with open('./data/news_df.pickle', 'rb') as file:\n",
    "    news_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(\"./data/grams/token_unigram_text\",'rb')  \n",
    "uni_lem_comb2 = pickle.load(fileObject)  ## load unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(\"./data/grams/bigram_text\",'rb')  \n",
    "bi_lem_comb2 = pickle.load(fileObject)  ## load unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(\"./data/grams/trigram_text\",'rb')  \n",
    "tri_lem_comb2 = pickle.load(fileObject)  ## load unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [democrats, united, states, house, representat...\n",
       "1    [british, steel, ordered, compulsory, liquidat...\n",
       "2    [iranian, foreign, minister, javad, zarif, war...\n",
       "3    [muslim, women, walk, near, burnt, car, jakart...\n",
       "4    [big, question, eu, vote, well, far, right, fi...\n",
       "5    [file, feb, file, photo, virginia, gov, ralph,...\n",
       "6    [file, april, file, photo, conductor, zubin, m...\n",
       "7    [file, feb, file, photo, boston, bruins, goali...\n",
       "8    [speaker, house, nancy, pelosi, calif, speaks,...\n",
       "9    [file, wednesday, jan, file, photo, prize, win...\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_lem = pd.Series([x for x in uni_lem_comb2])\n",
    "uni_lem[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    democrats united_states house_representatives ...\n",
       "1    british_steel ordered compulsory liquidation t...\n",
       "2    iranian_foreign minister_javad zarif warned un...\n",
       "3    muslim women walk near burnt car jakarta indon...\n",
       "4    big question eu vote well far_right file satur...\n",
       "5    file_feb file_photo virginia_gov ralph_northam...\n",
       "6    file april file_photo conductor zubin mehta is...\n",
       "7    file_feb file_photo boston bruins goalie tuukk...\n",
       "8    speaker_house nancy_pelosi calif speaks report...\n",
       "9    file wednesday jan file_photo prize_winning ke...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lem = pd.Series([x for x in bi_lem_comb2])\n",
    "bi_lem[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    democrats united_states house_representatives ...\n",
       "1    british_steel ordered compulsory liquidation t...\n",
       "2    iranian_foreign minister_javad_zarif warned un...\n",
       "3    muslim_women walk near burnt car jakarta indon...\n",
       "4    big_question eu vote well far_right file satur...\n",
       "5    file_feb_file_photo virginia_gov_ralph_northam...\n",
       "6    file april file_photo conductor zubin mehta is...\n",
       "7    file_feb_file_photo boston bruins goalie tuukk...\n",
       "8    speaker_house nancy_pelosi_calif speaks report...\n",
       "9    file wednesday jan file_photo prize_winning ke...\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_lem = pd.Series([x for x in tri_lem_comb2])\n",
    "tri_lem[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the Text into Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_vector(text):\n",
    "    text = text.split()\n",
    "    vector = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            vector.append(model.word_vec(i))\n",
    "        except:\n",
    "            pass\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_text = tri_lem.apply(infer_vector)\n",
    "values_of_erros = [i for i,x in enumerate(vec_text) if x == []]\n",
    "bad_series = vec_text.index.isin(values_of_erros)\n",
    "vector = vec_text[~bad_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec_text = []\n",
    "for row in vector:\n",
    "    vec_text.append(np.mean(row,axis=0))\n",
    "\n",
    "vec_text = pd.Series(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.012582379, 0.04536337, 0.031114908, 0.07705...\n",
       "1    [0.02133775, 0.026488008, 0.02957555, 0.040369...\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_text[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_text = pd.DataFrame(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 985 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vec_text = pd.DataFrame(vec_text[0].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012582</td>\n",
       "      <td>0.045363</td>\n",
       "      <td>0.031115</td>\n",
       "      <td>0.077060</td>\n",
       "      <td>-0.068374</td>\n",
       "      <td>-0.033830</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>-0.039497</td>\n",
       "      <td>0.089389</td>\n",
       "      <td>0.031833</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049665</td>\n",
       "      <td>-0.042488</td>\n",
       "      <td>-0.035812</td>\n",
       "      <td>0.031589</td>\n",
       "      <td>-0.023756</td>\n",
       "      <td>-0.045757</td>\n",
       "      <td>0.021921</td>\n",
       "      <td>-0.053957</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.049864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021338</td>\n",
       "      <td>0.026488</td>\n",
       "      <td>0.029576</td>\n",
       "      <td>0.040370</td>\n",
       "      <td>-0.034419</td>\n",
       "      <td>-0.037616</td>\n",
       "      <td>0.039927</td>\n",
       "      <td>-0.050052</td>\n",
       "      <td>0.116378</td>\n",
       "      <td>0.054397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113467</td>\n",
       "      <td>0.041973</td>\n",
       "      <td>-0.088883</td>\n",
       "      <td>0.037254</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.026207</td>\n",
       "      <td>-0.000665</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>0.052724</td>\n",
       "      <td>-0.062033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016563</td>\n",
       "      <td>0.059617</td>\n",
       "      <td>0.066829</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>-0.068136</td>\n",
       "      <td>-0.002744</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>-0.116771</td>\n",
       "      <td>0.074068</td>\n",
       "      <td>0.078070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038157</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>-0.050216</td>\n",
       "      <td>0.033304</td>\n",
       "      <td>-0.031910</td>\n",
       "      <td>-0.003213</td>\n",
       "      <td>-0.009037</td>\n",
       "      <td>-0.034112</td>\n",
       "      <td>0.037966</td>\n",
       "      <td>0.025232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.057837</td>\n",
       "      <td>0.039482</td>\n",
       "      <td>0.055657</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>-0.030582</td>\n",
       "      <td>-0.040403</td>\n",
       "      <td>-0.026342</td>\n",
       "      <td>-0.080705</td>\n",
       "      <td>0.070010</td>\n",
       "      <td>0.042793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080221</td>\n",
       "      <td>-0.039261</td>\n",
       "      <td>-0.069008</td>\n",
       "      <td>-0.004207</td>\n",
       "      <td>-0.021162</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.021799</td>\n",
       "      <td>-0.055461</td>\n",
       "      <td>0.036999</td>\n",
       "      <td>0.041280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027191</td>\n",
       "      <td>0.049305</td>\n",
       "      <td>0.038407</td>\n",
       "      <td>0.109705</td>\n",
       "      <td>-0.074799</td>\n",
       "      <td>-0.051456</td>\n",
       "      <td>-0.017526</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>0.078649</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059277</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>-0.063211</td>\n",
       "      <td>0.023837</td>\n",
       "      <td>-0.032217</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>-0.002150</td>\n",
       "      <td>-0.019368</td>\n",
       "      <td>0.041907</td>\n",
       "      <td>0.026303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.012582  0.045363  0.031115  0.077060 -0.068374 -0.033830  0.030153   \n",
       "1  0.021338  0.026488  0.029576  0.040370 -0.034419 -0.037616  0.039927   \n",
       "2  0.016563  0.059617  0.066829  0.081169 -0.068136 -0.002744  0.003846   \n",
       "3  0.057837  0.039482  0.055657  0.062033 -0.030582 -0.040403 -0.026342   \n",
       "4  0.027191  0.049305  0.038407  0.109705 -0.074799 -0.051456 -0.017526   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0 -0.039497  0.089389  0.031833    ...    -0.049665 -0.042488 -0.035812   \n",
       "1 -0.050052  0.116378  0.054397    ...    -0.113467  0.041973 -0.088883   \n",
       "2 -0.116771  0.074068  0.078070    ...    -0.038157  0.020842 -0.050216   \n",
       "3 -0.080705  0.070010  0.042793    ...    -0.080221 -0.039261 -0.069008   \n",
       "4 -0.099109  0.078649  0.067500    ...    -0.059277  0.008430 -0.063211   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.031589 -0.023756 -0.045757  0.021921 -0.053957  0.020599  0.049864  \n",
       "1  0.037254  0.009839  0.026207 -0.000665 -0.000213  0.052724 -0.062033  \n",
       "2  0.033304 -0.031910 -0.003213 -0.009037 -0.034112  0.037966  0.025232  \n",
       "3 -0.004207 -0.021162  0.000116 -0.021799 -0.055461  0.036999  0.041280  \n",
       "4  0.023837 -0.032217  0.002827 -0.002150 -0.019368  0.041907  0.026303  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving vec_text of trigrams:\n",
    "filename = './data/grams/vec_text_trigram.sav'\n",
    "pickle.dump(vec_text, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text and response to array \n",
    "y_response = news_df.Not_Real_or_Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_series_news_df = y_response.index.isin(values_of_erros)\n",
    "y_response = y_response[~bad_series_news_df].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving y_response trigrams:\n",
    "filename = './data/grams/y_response.sav'\n",
    "pickle.dump(y_response, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
